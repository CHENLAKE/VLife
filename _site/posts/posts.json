[
  {
    "path": "posts/2022-01-28-best-practice-in-power-bi/",
    "title": "For Beginners in Power BI",
    "description": "There are 3 scopes of knowledge involved in delivering a piece of  Data Visualization task :1.\tData Analytical knowledge --- understand the power of data ( statistics, data transformation, machine learning models etc); 2.\tData Visualization knowledge --- use apt charts /graphs to tell the story you want.3.\tTools related knowledge --- choose a media to formalize the work ( pencil & paper, excel, Power BI, Tableau etc. ).Today, I am discussing the 3rd scope, more specifically, Power BI.",
    "author": [
      {
        "name": "V.LC.LAKE",
        "url": {}
      }
    ],
    "date": "2022-01-28",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nETL :Power Query Editor\r\nModelling\r\nDAX Measures\r\n\r\n\r\n#TOC {\r\n  color: #873e23;\r\n  font-family: Agency FB;\r\n  font-size: 20px;\r\n}\r\nbody{\r\n  color: #873e23;\r\n}\r\n\r\n\r\n\r\nETL :Power Query Editor\r\nModelling\r\nDAX Measures\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-28-best-practice-in-power-bi/pics/cover.png",
    "last_modified": "2022-01-28T20:39:02+08:00",
    "input_file": "best-practice-in-power-bi.knit.md"
  },
  {
    "path": "posts/2022-01-10-pandas-essence/",
    "title": "Pandas Essence",
    "description": "Pandas is one of the most popular liabrary for python programming langauge to deal with data transformation and preparation as well as machine learning. It is impossible to remember all methods available, but definitely useful and wise to know the essence.",
    "author": [
      {
        "name": "V.LC.LAKE",
        "url": {}
      }
    ],
    "date": "2022-01-10",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nData Structure Attributes\r\nMethods\r\npd.Series\r\nCreate\r\nSort\r\nRemove Item(s)\r\nReplace items\r\nOperate on Items\r\n\r\npd.DataFrame\r\nAccess by either Col-or-Row\r\nAccess by both Col-and-Row\r\nData Selection with Filtering\r\nSort\r\nDrop Row/Column\r\n\r\nSeries.apply() V.S DF.apply()\r\nDataFrame.apply( )\r\nDataFrame.applymap( )\r\n\r\n\r\nDocumentation\r\n\r\n\r\n#TOC {\r\n  color: #3483eb;\r\n  font-family: Agency FB;\r\n  font-size: 20px;\r\n}\r\nbody{\r\n  color: #3483eb;\r\n}\r\n\r\n\r\n\r\nFigure 1: Python ANd Data AnalysiS ?\r\n\r\n\r\n\r\nData Structure Attributes\r\nBasically, numpy vector/ matrix provides the data to which pandas Series/ DataFrame adds the index and columns(names).\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nnp(np.array)\r\npd(pd.Series)\r\npd(pd.DataFrame)\r\n.dim\r\n.index\r\n.index\r\n.shape\r\n.values\r\n.columns\r\nFor df = pd.DataFrame,\r\ndf.index represents row-index, df.columns for column-index, both are index object.\r\ndf.index is element immutable, but can be replaced entirely.\r\nif want to set one column’s value as row-index : df.set_index( col_name, inplace = True )\r\nMethods\r\npd.Series\r\nCreate\r\n\r\npd.Series(val, index = )\r\n\r\nval can be: list, ndarray, dictionary( keys to be index), scalar (replicate to match index size )\r\nIndex can be duplicates, non-numbers\r\nSort\r\n\r\npd.Series.sort_values( ascending = , inplace = )\r\n\r\nRemove Item(s)\r\n\r\npd.Series.drop( index/ [ indices ] )\r\nE.g., df[‘gender’].drop( [3, 4] )\r\n\r\nReplace items\r\n\r\npd.Series.map( mapping )\r\npd.Series.replace( mapping )\r\n\r\nmapping is in form of dictionary, ‘key’ is replaced by ‘value’.\r\nif item in series not found in the mapping ( key ):\r\nseries.map() turn item (in original Series) into NaN;\r\nseries.replace() leaves the item unchanged\r\n\r\nOperate on Items\r\n\r\nSeries.apply( func_name )\r\n\r\npd.DataFrame\r\nAccess by either Col-or-Row\r\nColumn View\r\n\r\nsingle column : df.col or df[ col ]\r\nmulti-columns: df[ [col1, col2, col3…] ]\r\n\r\nRow View\r\n\r\nby row_number :\r\nsingle-row : df.iloc[ row_num ]\r\nmulti-rows:\r\nusing list: df.iloc[ [ r_#1, r_#2…] ] or using slicing: df.iloc[ slicing ]\r\n\r\nnotice: slicing refers to start_row_num: end_row_num, end row_num excluded\r\n\r\nby row_name: df.loc[ row_name ]\r\nwhere row_name could be : single row_name, list of row names, or slicing\r\n\r\nnotice : slicing refers to start_row_name : end_row_name, where end_row_name is included\r\nAccess by both Col-and-Row\r\nby both Col_num AND Row_num\r\n\r\ndf.iloc[ col_num, row_num ]\r\nwhere clo_num, row_num can be: single number/ list/ slicing\r\n\r\nby both Col_name AND Row_name\r\n\r\ndf.loc[ col_name, row_name ]\r\nwhere clo_num, row_num can be: single number/ list/ slicing\r\n\r\nData Selection with Filtering\r\nfilter_mask: similar to Boolean masking\r\ndf[ col ] 的Logic operation, 可多个: and \" & “，or” | “，not” ~ \" .\r\n\r\nft = ( df[‘age’] >= 10) & (df[‘height’] < 158)\r\n\r\nthis filter_mask returns a Boolean Series whose index are the same as df.index\r\n\r\n\r\n\r\nSyntax of data selection\r\ndo not specify Columns (filtered index for all columns ):df[ filter_mask ] or df.loc[ filter_mask ]\r\n\r\n\r\n\r\nspecify columns ( filtered index with selected columns ):df.loc[ filter_mask, [ cols ] ]\r\n\r\n\r\n\r\nSort\r\nSort Index\r\n\r\ndf.sort_index( inplace = True )\r\n\r\nSort Value\r\n\r\ndf.sort_value( by = [col_x, col_y, col_n ], ascending = [ True, False, True], inplace = True )\r\n\r\nDrop Row/Column\r\nremove rows: df.drop( index /[ indices ] )\r\n\r\n\r\n\r\nremove columns: df.drop( col_, aixs = 1 )\r\nor aixs = ‘columns’\r\n\r\n\r\n\r\nnotice: inplace parameter can be used to change the original Series/DataFrame\r\nSeries.apply() V.S DF.apply()\r\ns.apply( func_name )\r\napply function to each element in the Series\r\ndf.apply( func_name )\r\nfunction applies on each column or each row of the DataFrame\r\nwhere func_name can be : built-in name / self defefined function name / lambda\r\n\r\n\r\n\r\n\r\n\r\n\r\nresult returns a Series for both s.apply() or df.apply()\r\nDataFrame.apply( )\r\ndf.apply( func )\r\napply function column by column\r\ndf.apply(func, axis =1)\r\nrow by row\r\nresult returns a Series\r\nDataFrame.applymap( )\r\nfunction applies to each element in the DataFrame\r\nresult is still a DataFrame\r\n\r\n\r\n\r\nDocumentation\r\nhttps://pandas.pydata.org/pandas-docs/stable/reference/index.html\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-10-pandas-essence/pics/pandas.jpg",
    "last_modified": "2022-02-20T10:26:04+08:00",
    "input_file": "pandas-essence.knit.md"
  },
  {
    "path": "posts/2022-01-09-sharpen-jupyter-notebook/",
    "title": "Sharpen Jupyter Notebook",
    "description": "An artisan must sharpen her tools before she is to do the work not only well but also blissfully. Jupyter notebook is a powerful tool to learn python coding(local machine) as well as data science, AI(colab on cloud). Python course I took from university only covered python per se which was a bit of pity.",
    "author": [
      {
        "name": "V.LC.LAKE",
        "url": {}
      }
    ],
    "date": "2022-01-09",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nInstallation\r\nRun JN\r\nthrough Conda\r\nChange Root Path\r\n\r\nKnow Jupyter Notebook\r\nBasics\r\ndir()\r\nShutdown JN\r\nCheck Keyboard Shortcuts\r\n\r\nLife-easing Things\r\nShortcuts\r\nCommands\r\nExtension: Hinterland\r\nExtension: Python Markdown\r\n\r\nExport JN to PDF\r\nPackages to Install\r\nUnexpeted Error\r\nSolutions\r\n\r\nMake JN a code-free document\r\nResources\r\n\r\n\r\n#TOC {\r\n  color: #7B1FA2;\r\n  font-family: Agency FB;\r\n  font-size: 20px;\r\n}\r\nbody{\r\n  color: #7B1FA2;\r\n}\r\n\r\n\r\n\r\nInstallation\r\nHighly recommended to install it through Miniconda or Anaconda, since they are packed with Python so that you can skip installing python then associating it as the interpreter or kernel. For instance, if go to Miniconda’s page, you can directly choose the right Python version to download Miniconda.\r\n\r\n\r\n\r\nFigure 1: Windows Installer for Miniconda with Specific Python Version\r\n\r\n\r\n\r\nRun JN\r\nthrough Conda\r\nWhile my studying python in University, the professor skipped the tool part totally. For a long time, the way I run Jupyter Notebook is through running Anaconda’s Navigator, which is restrictive in terms of the root path or how flexible you want to manage your files.\r\n\r\n\r\n\r\nFigure 2: Run Jupyter Notebook through Conda’s Navigator UI\r\n\r\n\r\n\r\n\r\nby default, Conda will be installed in C:\\Users\\xx . The root path of JN will be the same if you open it through Conda’s Navigator.\r\n\r\nChange Root Path\r\n\r\n\r\n\r\nFigure 3: Open Conda’s Shell\r\n\r\n\r\n\r\nopen AnaConda Powershell Prompt.\r\nchange the directory to wherever you want to act as the root path.\r\nrun command: jupyter notebook.\r\n\r\n\r\n\r\nFigure 4: Run Jupyter Notebook through Conda’s Shell\r\n\r\n\r\n\r\nonce the JN is open in web browser, tree tells it is in root now, where you can start New files or folders, of which any action will be shown exactly the same in the file system.\r\n\r\n\r\n\r\nFigure 5: Set any Directory as Jupyter Notebook’s Root path\r\n\r\n\r\n\r\nKnow Jupyter Notebook\r\n\r\nThese are not official definition of JN, but some personal understandings which helped me in learning and working\r\n\r\nBasics\r\nweb-based computational environment for creating notebook documents (formally ipython notebook, that’s why its extension is .ipynb. Btw, the name jupyter is said to refer to 3 languages: Ju = Julia, Pyt = Python, the 3rd is R ).\r\na notebook is a JSON document( you can see it if you use any other python editor such as Spyder to open .ipynb file).\r\nJupyter Kernel is the program handling the relevant requests and response.\r\n\r\n\r\n\r\nFigure 6: Kernel -Python 3\r\n\r\n\r\n\r\ncodes or texts are written inside Cell of which there are 2 types: Code, Markdown. If you click any cell, the type will be shown here.\r\n\r\n\r\n\r\nFigure 7: Cell Type\r\n\r\n\r\n\r\ndir()\r\nby Python kernel, a notebook is run together with a python instance, which maintains all the variables and functions ever executed in code cells within a notebook, even after the cell is deleted. Those variables will stay there until they are explicitly deleted or shutdown the notebook (kernel).\r\n\r\nuse dir() in code cell can check what variables and methods are available in the current python instance.\r\nuse ‘del var’ to explicitly delete a variable from current python instance.\r\n\r\nnotebook will only care about the sequence of execution rather than the sequence a cell is positioned ( e.g.,insertion ).\r\nShutdown JN\r\nclosing the tab dose not equate shutting it down, as python will still run at the backend, tracking all variables and methods in the memory, when you reopen it and run a code cell, the execution order continues.\r\n\r\nso , the notebook continues running until you explicitly shut it down.\r\n\r\n\r\n\r\n\r\nFigure 8: Shutdown Jupyter Notebook\r\n\r\n\r\n\r\nCheck Keyboard Shortcuts\r\nin a .ipynb file, you can check all the shortcuts key under tab \" Help \".\r\n\r\n\r\n\r\nFigure 9: check shortcuts\r\n\r\n\r\n\r\nLife-easing Things\r\nShortcuts\r\nCOMMAND MODE\r\npress ‘Esc’ to enable\r\ninsert one cell bf current cell\r\n‘a’\r\ninsert one cell aft current cell\r\n‘b’\r\ndelete current cell\r\n‘dd’(hit ‘d’ twice)\r\nchange current cell to Markdown cell\r\n‘m’\r\nsave notebook\r\n‘s’\r\nEDIT MODE\r\npress ‘Enter’ to enable\r\nrun cell, select below\r\n‘shift’ + ‘Enter’\r\nundo\r\n‘ctrl’ + ‘z’\r\ncode completion or indent\r\n‘tab’\r\nCommands\r\nto check the directory of current notebook: ls\r\n\r\n\r\n\r\nFigure 10: List contents of directory of current Jupyter Notebook\r\n\r\n\r\n\r\nto run bash/terminal commands in code cell: by starting with ! , for example,\r\n\r\n! pip install seaborn\r\n\r\nExtension: Hinterland\r\nOne of my favorite extension of JN is “Hinterland” which will display all the possible commands when you are typing and allow you to read the detailed document of the commands.Other interesting extensions\r\nInstallation\r\nrun two commands one by one in cmd terminal or in JN (with ! ), detailed doc check Nbextensions.\r\n\r\npip install jupyter_contrib_nbextensions    \r\njupyter contrib nbextension install --user   \r\n\r\nrestart JN,enable the extensions desired (Hinterland )\r\n\r\n\r\n\r\nFigure 11: Nbextensions\r\n\r\n\r\n\r\nUsage\r\nread documentation of the commands when type code in JN.\r\n\r\n\r\n\r\nFigure 12: Check Doc of Command\r\n\r\n\r\n\r\n\r\nwhen the curser is in the bracket of the command, key in Shift + Tab.\r\nclick the ’ +’  on the upper-right corner to expand the pane.\r\n\r\nExtension: Python Markdown\r\nAllows embedding Python-code in markdown cells, e.g, display a variable’s value produced by code cell.\r\n\r\n\r\n\r\nFigure 13: Example of Extension: Python Markdown\r\n\r\n\r\n\r\n\r\nThe installation method is the same as shown in \" Extension: Hinterland\".\r\nIf already installed, just need select then enable this Python Markdown.\r\n\r\n\r\n\r\n\r\nFigure 14: Extension: Python Markdown\r\n\r\n\r\n\r\nExport JN to PDF\r\nA Jupyter Notebook can be converted to a number of open standard output formats (HTML, LaTeX, PDF, Markdown, Python) through “Download as” in the web interface.\r\n\r\n\r\n\r\nFigure 15: Convert Jupyter Notebook to other formats\r\n\r\n\r\n\r\nPackages to Install\r\nTo make it work, nbcovert library and its underlying packages need to be installed. read detail\r\nstep\r\nlibrary\r\nmethod\r\ncomment\r\n1\r\nnbconvert\r\npip install nbconvert\r\nshell cmd\r\n2\r\nPandoc\r\nhttps://pandoc.org/installing.html\r\nwindows installation\r\n3\r\nTex ( MikTex for windows)\r\nhttps://miktex.org/download\r\nwindows installation\r\n4\r\nPyppeteer Chromium\r\npip install nbconvert[webpdf]\r\nshell cmd\r\nUnexpeted Error\r\nafter going through the 4 steps above and we happily trying to convert a notebook to pdf, we may unexpectedly see this message:\r\n\r\n\r\n\r\nFigure 16:  Error Msg\r\n\r\n\r\n\r\nSolutions\r\ngo to This PC > Advanced system settings > Environment Variables, add in the path of MikTex executable.\r\n\r\n\r\n\r\nFigure 17: Add MikTex into Environment Variable\r\n\r\n\r\n\r\nrestart computer, relaunch JN, retry to “Download as .pdf”, then several ‘Package Installation’ windows will pop out, all clicking Install.\r\n\r\n\r\n\r\nFigure 18: a Series of Package Installation\r\n\r\n\r\n\r\nMake JN a code-free document\r\nThere might be times that you want to share the data analyzing results to your boss who does not need to know all the codes you use. You need hide the code.\r\nThere exists this hide_code extension for JN to selectively hide code, prompts and output with PDF and HTML exporting support, making a notebook a code free document for exporting or presenting.\r\nFollow two steps to make this extension work :\r\ngo to this hide_code extension  to see the demo and make sure you already meet the last Requirements part.\r\nNormally, pdfkit is already installed, you can try pip command in an open .ipynb file to check out.\r\n\r\n! pip install pdfkit\r\n\r\nNormally, wkhtmltopdf needs to be installed and Environment Variable may also need to be added after installation. Check wkhtmltopdf download\r\nrun 4 shell commands which are stated also in this hide_code extension  under “Jupyter Notebook Installation” section.\r\n\r\npip install hide_codejupyter nbextension install --py hide_code jupyter nbextension enable --py hide_code jupyter serverextension enable --py hide_code\r\n\r\nResources\r\nbuilt-in magic commands\r\nnbconvert: Convert Notebooks to other formats\r\nnbformat: Python API for working with notebook files\r\npapermill: parameterizing and executing Jupyter Notebooks\r\nnbextentions\r\nnbviewer: a way to share Jupyter notebooks via the browser\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-09-sharpen-jupyter-notebook/pics/isaac_.png",
    "last_modified": "2022-02-11T23:38:08+08:00",
    "input_file": "sharpen-jupyter-notebook.knit.md"
  },
  {
    "path": "posts/2021-12-17-report-with-r-markdown/",
    "title": "Report with R Markdown",
    "description": "Markdown can be used in many situations, such as writing report in jupyter notebook, creating distill blog in R studio or simply generating report in html, word or pdf format. Some tips & resources are commonly used.",
    "author": [
      {
        "name": "V.LC.LAKE",
        "url": {}
      }
    ],
    "date": "2021-12-17",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nYAML Header\r\nAdd Date Manually\r\nAdd Date Automatically\r\nAdd Table-of-Content & CSS\r\nCustomize fonts and colors\r\nAdd Parameters\r\n\r\nPlots & Vedios\r\nInsert pics\r\nInsert pic with R code chunk\r\nFigure arguments\r\nSet Options Globally\r\nInsert Vedio\r\n\r\nAdd Tables\r\nby R code chunk\r\nTable Generator\r\n\r\nCode Chunk\r\nOptions\r\nLoad Packages\r\nInteract with Python\r\n\r\n\r\nIn a R Markdown document, there are mainly 3 elements which are YAML Header, Text Body and Code Chunk.\r\nonline cheat sheet\r\nYAML Header\r\nAdd Date Manually\r\n\r\n\r\n\r\n---\r\ntitle: \"Untitled\"\r\nauthor: \"V LAKE\"\r\ndate: \"12/17/2021\"\r\noutput: html_document\r\n---\r\nAdd Date Automatically\r\n\r\n\r\n\r\n---\r\ntitle: \"Untitled\"\r\nauthor: \"V LAKE\"\r\ndate: \"2022-01-29\"\r\noutput: html_document\r\n---\r\n\r\n\r\n\r\n---\r\ntitle: \"Untitled\"\r\nauthor: \"V LAKE\"\r\ndate: \" 18:46:40 29 Jan,2022 \"\r\noutput: html_document\r\n---\r\nFormat Options\r\n\r\nNumeric:%H : Hour; %M : Minute; %S : Second; %Y or %y : Year; %m : Month; %d : DayText:%A or %a : Weekday; %B or %b : Month(e.g., January or Jan)\r\n\r\nAdd Table-of-Content & CSS\r\n\r\n\r\n\r\n---\r\ntitle: \"Untitled\"\r\nauthor: \"V LAKE\"\r\ndate: \" 12/17/2021 \"\r\noutput: \r\n  html_document:\r\n    toc: true\r\n    number_sections: true\r\n    toc_float: true\r\n    toc_depth: 3\r\n    css: style.css\r\n---\r\nCustomize fonts and colors\r\nCreate a text file and name it as style.css then link it in YAML header.\r\n.css file should be in the same directory as .Rmd file\r\n#TOC {\r\n  color: #B8860B;\r\n  font-family: Agency FB;\r\n  font-size: 16px;\r\n  border-color: #708090;\r\n}\r\n#header{\r\n  color:red;\r\n  opacity: 0.8;\r\n}\r\nbody{\r\n  color:green;\r\n  font-size: 10px;\r\n}\r\npre{\r\n  color:blue;\r\n  background-color: #eab676;\r\n}\r\n\r\n\r\n\r\nAdd Parameters\r\n\r\n\r\n\r\n\r\nAdd <name : value > under params: in YAML header\r\nInvoke the parameters in YAML header’s title, Text body and Code chunk.\r\n\r\n\r\n\r\n\r\nPlots & Vedios\r\nInsert pics\r\n<img src=\"pics/css.png\" />\r\n![text](image.jpg)\r\nInsert pic with R code chunk\r\n\r\n\r\nknitr::include_graphics(\"pics/css.png\")\r\n\r\n\r\n\r\n\r\nIf using this piece of command inside R code chunk to insert picture,do insert a space line between the text and code chunk, otherwise it will reset the format of later text to normal body, even you tried to format it as header or index.\r\nFrankly speaking, whenever your format doesn’t work as you expected, try insert a space line between different formats.\r\n\r\nFigure arguments\r\n\r\n\r\n\r\n\r\nFig Size\r\n1. fig.width= 5,fig.height =3 OR\r\n2. fig.dim = c(5,3) OR\r\n3. out.width =’ % ‘,out.height =’ %’Fig Alignment : fig.align = 'left'/'center'/'right'Fig Caption : fig.cap = 'xxx'\r\n\r\nSet Options Globally\r\nknitr::opts_chunk$set( setting, echo= TURE)\r\nsetting before echo = is the global settings\r\n\r\n\r\n\r\nInsert Vedio\r\nusing HTML5 video tag to insert local files.\r\n<video controls src=\"../images/animation.m4v\"> animation <\/video>\r\nAdd Tables\r\nby R code chunk\r\n\r\nUse kable( dataset, args ) in r code chunk.\r\narguments include:col.names = c(\"col1\",\"col2\",\"col3\" )align = \"ccc\"caption = \" \"\r\n\r\n\r\n\r\n\r\nTable Generator\r\nusing online table generator to input the data in the table, preview then copy the code to clipboard to avoid tedious insertion of hypens and pipes.\r\n\r\n\r\n\r\nCode Chunk\r\nOptions\r\n\r\n\r\n\r\n\r\nCode & Output Result \r\n1. include = FALSE : show nothing, but run in background .\r\ne.g., import library, dataset / setup global options/ exclude messages\r\n2. echo = FALSE : only show result ( no code ), e.g., plotting\r\n3. eval = FALSE : only show code ( not run, no actual impact by the code )\r\n4. collapse = FALSE : combine code & result together in same area (by default, they are separated )Message,error,warning Default Value:\r\n1. warning /message = TRUE\r\n2. error = FALSE ( if encounter error, stop knitting )\r\n\r\nLoad Packages\r\n\r\n\r\n\r\nInteract with Python\r\nLet’s make it super clear: R Markdown and knitr do support Python.The Python support in R Markdown and knitr is based on the reticulate package which allows two-way communication between Python and R.\r\n\r\nunfortunately, I prefer to use them separately. However, more info can be found here.\r\n\r\nIf only for report purpose for which we only want to show the python codes in an R Markdown document, we can add chunk header like this:\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-17-report-with-r-markdown/pics/specificDate.png",
    "last_modified": "2022-01-29T18:46:42+08:00",
    "input_file": "report-with-r-markdown.knit.md"
  },
  {
    "path": "posts/2021-12-06-cloud-api-journey/",
    "title": " Google Cloud API and POSTMAN",
    "description": "Resources of learning API& POSTMAN from basic concepts. \nSteps of setting up OAuth2.0 authentication for Google cloud API and how to send Google-DOC-AI processing request in POSTMAN. \nFinally, evaluate DocAI's performance on samples to get a statistical confidence interval for population's average accuracy rate (python code included).",
    "author": [
      {
        "name": "V.LC.LAKE",
        "url": {}
      }
    ],
    "date": "2021-12-06",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nAPI Basics\r\nPOSTMAN\r\nPostman OAuth2.0 authentication to use Google API\r\nSend a Google-DOC-AI Processing Request in POSTMAN\r\nTest DocAI API in Python\r\nGoogle Doc AI API References\r\n\r\nAPI Basics\r\nAPIs for beginners\r\n\r\n\r\n\r\nPOSTMAN\r\nPostman Beginner’s Course - API Testing\r\n\r\n\r\nQuery Parameter\r\n\r\nQuery Parameter is used to filter what API returned, usually by pointing out the field’s value of returned json.\r\n\r\n\r\n\r\n\r\nTests tab\r\n\r\nIf we want to check/test data of the returned Body, we could filter them in the Tests tab (won’t change the returned Body, but filter in the “Tests” tab).\r\n\r\n2.1. We can use Console.log () to see the results of Tests,also can set the result of interest as a global variable for later requests.\r\n\r\n\r\n\r\n\r\nOne of the advantage of using POSTMAN console.log() is that the returned Json file can be viewed in a very clear manner, since the field and value are displayed in different colors.\r\n\r\n\r\n\r\n\r\n2.2. pm.test()\r\npm.test( “description string”, callback function-what to test ) is for postman to test the request response.\r\n\r\n\r\n\r\n2.3. jump a request\r\n\r\n\r\n\r\n2.4. stop\r\n\r\n\r\n\r\nPostman OAuth2.0 authentication to use Google API\r\n\r\nUsing POSTMAN requires different authentication from using python code which uses Service Accounts and its json key, additional setups are needed to perform:\r\n1. Register POSTMAN as Authorised redirect URIs (web applications)\r\n2. Register postman as Authorised domains\r\n3. Add scope for specific API\r\n\r\nSetup Steps :\r\nGo to Google Cloud Platform and sign in your Google account, select the Project you are in.\r\nGo to APIs & Services and then to Credentials > Create OAuth Client ID\r\nSelect Web application for Application type and enter either https://oauth.pstmn.io/v1/browser-callback or https://oauth.pstmn.io/v1/callback for Authorized redirect URLs. This ensures the auth flow works for Postman on both desktop and web. Once you hit Create you will see Client ID and Client Secret - those two values are important (do NOT share with anyone) and we will need them later in Postman.\r\nNext go to OAuth consent screen and enter oauth.pstmn.io for Authorised domains. For Scope, and for Google cloud API, https://www.googleapis.com/auth/cloud-platform\r\n\r\n\r\n\r\n5. Go to postman, and edit the collection for the project. In Authorization tab > Type, choose oAuth 2.0, the go down to Configure New Token, fill in the information and click Get New Access Token.\r\nFor “Auth URL”: https://accounts.google.com/o/oauth2/auth\r\nFor “Access Token URL”: https://accounts.google.com/o/oauth2/token\r\nFor “Scope”: https://www.googleapis.com/auth/cloud-platform Then it will open up a web browser window asking for your authentication.\r\n\r\n\r\n\r\nFinally, token will be generated , as below. Those token may expire in some period, when it comes, just repeat the step 5 to get new token.\r\n\r\n\r\n\r\nWhen making request, just choose / inherit the Available Tokens.\r\n\r\n\r\n\r\nSend a Google-DOC-AI Processing Request in POSTMAN\r\ncheck official reference\r\n\r\n\r\n\r\nHTTP method and URL:\r\nPOST \r\nhttps://LOCATION-documentai.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID:process \r\nBody:\r\n{\r\n  \"skipHumanReview\": true,\r\n  \"inlineDocument\": {\r\n    \"mimeType\": \"application/pdf\",\r\n    \"content\": \"{{f4}}\"\r\n  }\r\n}\r\nHuman Review means Human In The Loop processor which enables human verification and corrections to ensure accuracy of data extracted by Human in the Loop processors before it is used in critical business applications. It provides a workflow and UI for humans (referred to as labelers in HITL) to review, validate and correct the data extracted from documents by Human in the Loop processors.\r\nmimiType is talking about the file formats DOC AI supports:'application/pdf' ,'application/json', 'image/tiff', 'image/gif'.\r\nfor content we need pass base64 encoding (ASCII string) of binary image data into JSON representations. Google also has its recommended ways to do it from which my code also borrowed the core.\r\n\r\nimport base64\r\n# Pass the image data to an encoding function.\r\ndef encode_image(image):\r\n  image_content = image.read()\r\n  return base64.b64encode(image_content)\r\n\r\nf4 =\"CI0000000041.pdf\"\r\nfile_dir =\"C:/V/Google AI doc/invoice_parser/\"\r\nPDF_PATH = file_dir + f1 # Update to path of target document\r\ntext_dir = file_dir +'base64.txt'\r\n# encode\r\nwith open(PDF_PATH, \"rb\") as image:\r\n    content = encode_image(image)\r\n# get content and use it in POSTMAN\r\nwith open(text_dir,\"wb\") as f:\r\n    f.write(content)\r\n  \r\n\r\n\r\nA good way to adduce the content({{f4}}) is to save it as a global variable\r\n\r\n\r\n\r\n\r\nTest DocAI API in Python\r\nTo evaluate this API’s performance on images of commercial invoice in customs clearance, I tested over 50 samples(.pdf or .tif files) using python code as below. The output will be in format of .xlsx for the convenience of further calculation of the confidence interval for population average accuracy rate.\r\n\r\n\r\n\r\n\r\n\r\nimport os\r\nimport pandas as pd\r\n\r\npd.set_option('max_colwidth', None)\r\npd.set_option('display.max_columns', None)\r\n\r\n\"\"\"\r\nrefer to  https://cloud.google.com/document-ai/docs/send-request#documentai_process_document-python;\r\nfunction : to extract feilds relavent to logistic business\r\n\"\"\"\r\nfrom google.cloud import documentai_v1 as documentai\r\ndef process_document_sample(\r\n    project_id: str, location: str, processor_id: str, file_path: str\r\n):\r\n    # You must set the api_endpoint if you use a location other than 'us', e.g.:\r\n    opts = {}\r\n    if location == \"eu\":\r\n        opts = {\"api_endpoint\": \"eu-documentai.googleapis.com\"}\r\n\r\n    client = documentai.DocumentProcessorServiceClient(client_options=opts)\r\n\r\n    # The full resource name of the processor, e.g.:\r\n    # projects/project-id/locations/location/processor/processor-id\r\n    # You must create new processors in the Cloud Console first\r\n    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\r\n\r\n    with open(file_path, \"rb\") as image:\r\n        image_content = image.read()\r\n\r\n    \"\"\" \r\n    Read the file into memory.  \r\n    'mime_type' can be 'application/pdf', 'image/tiff', 'image/gif', or 'application/json'\r\n    document = {\"content\": image_content, \"mime_type\": \"image/tiff\"}\r\n    \"\"\"\r\n    \r\n    #chose the right mime_type\r\n    if file_path[-3:] ==\"pdf\" :\r\n        mime ='application/pdf'\r\n    elif file_path[-3:] ==\"tif\":\r\n        mime = \"image/tiff\"\r\n    else:\r\n        print(\"mime_type not recognizable\")\r\n    \r\n    # Read the file into memory.\r\n    document = {\"content\": image_content, \"mime_type\": mime}\r\n    # Configure the process request\r\n    request = {\"name\": name, \"raw_document\": document}\r\n\r\n    # Recognizes text entities in the PDF document\r\n    result = client.process_document(request=request)\r\n\r\n    document = result.document\r\n\r\n    print(\"Document processing complete.{}\\n\". format(file_path))\r\n    \r\n    # customized Fields to Commercial Invoice of logistic industry \r\n    entities = document.entities\r\n    not_relevant_types =['line_item','vat','total_tax_amount','receiver_tax_id','supplier_tax_id',\r\n                         'supplier_iban','freight_amount', 'net_amount','supplier_email','supplier_website']\r\n    types,values,confidence = [],[],[]\r\n    \r\n    # Grab relavent fields < key/value > pair and their corresponding confidence scores.\r\n    for entity in entities:\r\n        if ( entity.type_ not in not_relevant_types ):\r\n            types.append(entity.type_)\r\n            values.append(entity.mention_text)\r\n            confidence.append(round(entity.confidence,4))\r\n        else:\r\n            pass \r\n    # print in tabular format. \r\n    df = pd.DataFrame({'Type': types, 'Value': values, 'Confidence': confidence})\r\n    return df\r\n\r\n# Google Cloud Platform info\r\njsonkeyDirectory = \"C:\\\\V\\\\Google AI doc\\\\xxx\"   \r\ngoogleProjectId = \"xx-ea-XXX-XXX-gcpgogleap-xxx\"\r\ngoogleJsonKey = jsonkeyDirectory + \"\\\\documentaikey.json\"\r\nlocation = 'eu' # Format is 'us' or 'eu'\r\nprocessor_id = 'xxxxxxxxxxxxx' # Create processor in Cloud Console, here is invoice parser, and HITL disabled\r\n\r\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = googleProjectId\r\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = googleJsonKey \r\n\r\n# Test all Samples in one direcotry\r\nsamples_dir =\"C:\\\\V\\\\Google AI doc\\\\6. Evaluation Framework\\\\50 CI samples\\\\\"\r\noutput_dir =\"C:\\\\V\\\\Google AI doc\\\\6. Evaluation Framework\\\\50 CI fields output\\\\\"\r\nos.chdir(samples_dir)\r\n\r\n# input each sample into processor and output a corresponding excel file. \r\nfor f in os.listdir():\r\n    file_path = samples_dir + f\r\n    # read in testing files names and test\r\n    df_fields = process_document_sample(googleProjectId, location, processor_id, file_path)\r\n    excel_name = output_dir + f[0:2]+'.xlsx'  # e.g:excel name will be \"01.xlsx\"\r\n    df_fields.to_excel(excel_name)\r\n    \r\nprint(\"All Files Test & Output COMPLETED !\")\r\n\r\nGoogle Doc AI API References\r\n\r\n\r\n\r\nOverviewHow to GuidesReference\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-06-cloud-api-journey/pics/api_basics.jpg",
    "last_modified": "2022-01-25T10:03:23+08:00",
    "input_file": "cloud-api-journey.knit.md"
  },
  {
    "path": "posts/2021-10-07-blockchain-and-smart-contract/",
    "title": "Blockchain and Smart Contract",
    "description": "Mabybe in one day, we can realize social covenant, not only in concept but also in execution, to reach a state of unprecedented fairness in human history.",
    "author": [
      {
        "name": "V.LC.LAKE",
        "url": {}
      }
    ],
    "date": "2021-10-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nBlockchain v.s. Protocol v.s. Token\r\nSmart Contract\r\nEthereum Turing Complete\r\nWhat’s special about Blockchain\r\n\r\n\r\n#TOC {\r\n  color: #A23EF4;\r\n  font-family: Agency FB;\r\n  font-size: 20px;\r\n}\r\nBlockchain v.s. Protocol v.s. Token\r\nBlockchain is a special (cryptographical blocks, chain, immutability, decentralization…) type of data structure or database implemented by a network of nodes who run on the same set of protocols (for example, Ethereum or Bitcoin or Neo etc.,). Its data entries are produced by nodes through a set of rules including consensus.\r\nOn top of those protocols, the token can be generated by a specific type of smart contract which use the blockchain to record each account’s balance and transaction records. Token can be used to pay for network fees, smart contract deployments and in dApp(decentralized application) purchases.\r\nSmart Contract\r\nSmart contract is just a jargon of blockchain, essentially, it is a piece of code, and more specifically, it resembles the class concept in python and many other programming languages. The main compositions are contract data and functions (methods to manipulate the data in the contract).\r\n\r\n\r\n\r\nOnce the contract is deployed, it is stored in a place (an address) on blockchain. And other users of this smart contract could use (call) those functions, then the data inside the contract are impacted as a result.\r\n\r\n\r\n\r\nEthereum Turing Complete\r\nFirst of all, it is talking about smart contract language Solidity for Ethereum. Solidity is Turing complete.\r\nSo, what dose it to do with Turing? Well because he designed the theoretical computer which can solve any computational problem, unlike the computing machines before him which were designed specifically for a certain problem, which means if you wanted to compute another set of mathematical problems, you need build another tailored machine. So such a computer which promises to solve any computational problem (though it doesn’t guarantee the time that will take: sooner or later) is regarded as Turing complete (TC).\r\nIf a programing language like Solidity is called TC, it means that you can solve any computational problem using this language. Practically speaking, one feature of a TC language is loop. Bitcoin script language is not TC by its designer, but Ethereum’s is, partly because Bitcoin is solely for payment while Ethereum’s designers want it to be the platform of dApps, which needs to empower its script language more.\r\nTo avoid people using loop maliciously on blockchain, Ethereum has adopted the idea of Gas to charge for each computing operation in the smart contract.\r\nWhat’s special about Blockchain\r\nFor applications (codes) running on a server, all clients (the front ends) are only sending requests then the codes are run on a central location (the server). So, the codes are stored and executed centrally.\r\nOn the other hand, blockchain technology enables codes stored and run in every node of its network (decentralized) and what’s amazing is that blockchain technology can make all those nodes work in quantum entanglement style, which means once the data on blockchain is successfully modified by one node (by Pow, Pos etc), the data stored in other nodes will be updated to reflect the change also. The result will be that the same world state (all information in blockchain at one time) is shared by all nodes in the network of this blockchain.\r\nThis kind of synchronization made by distributed nodes is achieved by the combination effects of the same set of consensus mechanism, cryptography standards as well as messaging protocols.\r\n\r\nYou can think of a picture that all nodes are surrounding a shared holy tablet ( the distributed ledger aka blockchain ) on which each entry is written after all nodes running the same script (smart contract)and agreeing on one final result (involve consensus and messaging each other). In the protection of HASH, history is created and witnessed by all and immutable once it is on the chain.\r\n\r\n\r\nFinally, if we let our imagination go wilder, we can put the law execution in smart contract to realize the idea of social covenant. What’s special about blockchain technology is that it enables the same rule applies to everyone, not only on concept but also on execution, since the terms will be automatically checked against and trigger the responsive punishment or reward` .\r\n\r\nIn this sense, blockchain will make us to reach a state of unprecedented fairness.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-07-blockchain-and-smart-contract/pic/contract.png",
    "last_modified": "2021-12-18T11:51:43+08:00",
    "input_file": "blockchain-and-smart-contract.knit.md"
  },
  {
    "path": "posts/2021-09-10-from-ideas-to-execution/",
    "title": "From Ideas to Deliverable Works",
    "description": "Imagine a ship confronting a capricious ocean, who needs the ability to be adaptive-to-change? The crew members on the ship? Or The captain?  Neither. It is the ship that needs to be able to change swiftly.",
    "author": [
      {
        "name": "V.LC.LAKE",
        "url": {}
      }
    ],
    "date": "2021-09-10",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n1. A Bad Experience\r\nA few years ago, I had this exhausted experience in making changes in a small real estate development company. At the beginning as a member of senior management, the initiative to deploy new strategy for the project and company was all exciting since the plan/ vision/ ideas from board was quite appealing, not only to senior managers, but also to government and medias. Why? Because those ideas are “creative”, new technology employing, sound future-oriented and promising. But the reality is those ideas never realize.\r\nWhen it comes to internal execution and deliveries, that hot passion met with cold reaction from employees. In our eyes, most of them were unwilling to accept new ideas, slow to changes, not responsive to the use of new tools… But in their eyes (this is the retrospective view), senior managers came up with new ideas everyday and enjoyed the luxury of freeing their imagination and widening their vision.\r\nIn staffs’ eyes, top managers were good at pointing out the destination for a ship in the ocean but left the crews to figure out how to get there. However, there are infinite number of paths possible to lead to that destination and before an executable plan forms, the first reaction staffs would take usually is to wait and maintain the status quo —以不变应万变 in Chinese. In managers’ eyes, this is the signature of being slow or even reluctant to change. But what are they waiting for? For things/ tasks become clearer.\r\n1.1 Who’s to blame?\r\nI can’t blame each side. Can’t blame the top managements for having too many ideas, since that is their job and value.\r\nNeither can I blame the employees, since when they signed employment contract, they were agreed to serve specific roles with specific skills (most likely there was no itemized job requirement for being-adaptable-to-change, since no one would in advance know what is going to change in the future and what to adapt to ? )and they normally do well in completing a clearly defined task.\r\nTo V, there is a gap need to be filled. A gap between manager’s innovative (but sometimes inevitably vague) idea and an executable path/ plan for staffs to carry out professionally.\r\nTherefore, a middleware / middleman is needed, to have the talent and responsibility of drawing out the path, leaving the employees focusing on their own fields and tasks only.\r\n2. A surprisingly effortless but successful project experience\r\nDuring master program, for almost each course we took, we were asked to form a team and a problem that need to be solved, then use the skill and knowledge learnt in class to form a detailed plan even model to solve it. (Great program, thanks to Singapore Management University)\r\nThose projects were not easy though, since I didn’t know those team members before ,unaware of their mindsets and capabilities. Most of the time, I felt I was trying my best but still couldn’t get a great result/ project performance.\r\nI tried to force myself to accept the notion that teamwork won’t be easy, until I finished a data analytic project with one classmate and got it accepted by JMP Discovery Summit Americas 2021. The most unbelievable part is that that is the most comfortable and effortless project for me till now, but it achieves the best result! (easy doesn’t mean the project has no intellectual value, effortless doesn’t mean we succeeded due to luck)\r\n2.1 Why least effort but most effective?\r\nUpon completion of that project in May 2021, I couldn’t help giving gratitude to Yongkai (my teammate) since I felt that I didn’t “do” the work much (e.g.,run the model or write report) but contribute ideas and discussions along the way (e.g.,frame problem statement, discuss the approach to tackle the problem, raise questions on his analysis, read the final report and give feedback )\r\nSurprisingly, Yongkai conveyed his thanks to me for all my ideas and questions raised in the project. He said he is the kind of person good at effective execution but he needs clear directions, once he knows what to do next, he could be very efficient to deliver. He said because his energy focused on finding solutions for the problems occurring during doing, he was not good at thinking out of the box or thinking differently.\r\nAll in all, it turns out the best outcome comes not from “hard” work but from the coincidence that our thinking habits fit each other’s perfectly, the fact that we only need to contribute in what we are good at made everyone’s work effortless but fruitful.\r\n2.2 Key takeaways from this experience\r\nEveryone has his/her own way of thinking formed by his or her entire life. If possible, matching team with complementary Thinking Style, for example,\r\nConceptual ：Visionary/ Intuitive about ideas/ Enjoys the unusual/ Learns by experimenting\r\nProcedural : Likes guidelines/ Predictable/ Cautious of new ideas/ Learns by doing\r\n\r\nThere is no better or worse thinking style, since any work needs both thinking and doing. The fact that in general we are comfortable and proficient at only one style,we need appreciate each other to be able to deliver a piece of fine work..\r\n3. What about the ability to change\r\nIn a fast-pacing ever-changing business world, many believe it is paramount to have the ability to change. Again, sounds appealingly correct, but think deeply, who is the subject here?\r\nImagine a ship confronting a capricious ocean, who needs the ability to be adaptive-to-change? The crew members on the ship? Or The captain? Neither. It is the ship that needs to be able to change swiftly. Each staff on the contrary, needs specialize in their own task.\r\n\r\nThis is the power of system behavior (to put it simply, 1+1 > 2).\r\nEach water molecule in the ocean needs to perform the function no more than what a H2O molecule can do. It is the system that forms the totally disparate existence – the ocean.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-10-from-ideas-to-execution/pic/oceanship.jpg",
    "last_modified": "2022-01-09T23:52:32+08:00",
    "input_file": "from-ideas-to-execution.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "IT IS More Than Memory",
    "description": "Writing today to amaze self in the future.",
    "author": [
      {
        "name": "V.LC.LAKE",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\nKiki’s 1st official hand-written English. 如此美，如此美好。\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/welcome/pic/to_grace.jpg",
    "last_modified": "2021-09-10T22:43:51+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-09-steps-to-create-distill-blog/",
    "title": "Distill Blog with R Studio",
    "description": "Wrap your head around the relationships among Distill-Rmarkdown-Git-Github-Netlify. Be familiarized with the commands necessary in Rstudio to create blog and to make new posts.",
    "author": [
      {
        "name": "V.LC.LAKE",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nClear Buzz-words first !\r\nDistill References\r\n\r\nGetting Started\r\nStructure of Dstill Blog in Rstudio\r\nSteps & Commands\r\nV 说\r\n\r\n\r\n#TOC {\r\n  color: #E3180D;\r\n  font-family: Agency FB;\r\n  font-size: 20px;\r\n}\r\nClear Buzz-words first !\r\nDistill is a package in R to use R Markdown to creat professional-looking blogs. It takes care of all the html-webpage’s construction, leaving us only need to focus on how to use Rmarkdown.\r\nGit is a version control application which helps to track all the modifications of the blog posts.It is quite convenient if we directly use it in Rstudio.\r\nGithub is a cloud app of Git. Commit-in-Git, then push-to-Github are the successive steps each time we make change to the blog project.\r\nNetlify is where we can share the blogs with the rest of the world. Each blog project will be assigned a site name in its own domain and most importantly it can connect with Github therefore seamlessly work with Github. Once we deploy the blog project in Netlify for the first time, each time we make changes or creat new posts, we only need proceed Commit-Push (Git-Github), Netlify will automatically detect the change and update our web pages.\r\nDistill References\r\n1 Distill for R Markdown\r\n2. Reference Guide\r\n3. distilltools\r\n4. The disdillery–customize your sites\r\n5. Personalizing the distill template\r\nGetting Started\r\ninstall Git\r\nconfigure Git in Rstudio\r\n\r\nregister Github\r\nsign up Netlify and connect with your Github\r\ninstall R packages in R Studio\r\n\r\n\r\n\r\nDistill for R Markdown\r\nusethis\r\nStructure of Dstill Blog in Rstudio\r\n\r\n\r\n\r\nAfter creating a distill blog project(VLife), the structure of the blog is automatically created, including a index.Rmd,a about.Rmd and a welcome.Rmd ,two folders(_posts, _site) as well as a yaml file(_site.yml)\r\n_posts is where we create new blog posts in Rstudio\r\n_site is where Netlify look for the html pages it needs to update\r\n\r\nSteps & Commands\r\n参考Dan心目中的大神 Prof KAM Tin Seong 教你 Building a blog with distill for R Markdown\r\nIn Rstudio, New Project> Distill Blog\r\n\r\n\r\n\r\n注意在【New Project Wizard】,put the directory(project)in subdirectory which has the same name as Github name(e.g.,CHENLAKE).\r\nEdit and Kit blog page (e.g.,index.Rmd) to see the blog html\r\nPublish\r\n3.1. Git\r\nIn console,input command usethis::use_git()\r\nIf it is the 1st time to use Git for a newly created project, select <not now> for <commit>, <yes> for <restart Rstudio>\r\nElse, in Git pane, select all the modified files then commit\r\ncommit\r\n3.2. Github\r\nIf it is the first time to use Github for a newly created project, in console, input command usethis::use_github()\r\nElse, in Git pane, select push\r\n\r\n3.3. Netlify\r\nIt only needs to deploy once (the first time).Point the <Publish directory> to <_site>\r\n\r\n\r\n\r\n\r\nCreat new post\r\ninput console command distill::create_post(“Post name”)\r\na new folder will appear in <_posts> folder followed by Commit and Push in Git pane.\r\n\r\n\r\n\r\n\r\nV 说\r\n\r\nProf Kam is the one who keeps reminding us to record or document life.\r\nI agree.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-01-09T22:57:03+08:00",
    "input_file": "steps-to-create-distill-blog.knit.md"
  }
]
